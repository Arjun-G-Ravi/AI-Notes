# NLP
Natural Language Processing is a field of study focused on the interaction between computers and human (natural) languages. The ultimate objective of NLP is to read, decipher, understand, and make sense of the human language in a valuable way.

# NLP CONCEPTS

## 1. Text normalization
Text normalization in Natural Language Processing (NLP) refers to the process of converting a piece of text into a standardized form. This involves correcting spelling errors, expanding abbreviations and contractions, removing punctuation marks, converting all characters to lowercase or uppercase, and applying other transformations that make the text easier to analyze and compare.

## 2. Tokenisation
Converting text data into tokens.
- Should "New York" be considered as one token or two? Should punctuations be considered to be separate tokens? etc. all depends on the application. 

## 3. Stemming
Reducing words to their base or root form, by chopping off parts of words.
Eg: SnowBallStemmer(violent stemmer), PorterStemmer(simple stemmer)

## 4. Lemmatization
It is stemming performed by consulting the context of the text

## 5. Regular Expression
Formally, a regular expression is an algebraic notation for characterizing a set of strings. Regular expressions are particularly useful for `searching in texts, when we have a pattern to search for` and a corpus of texts to search through.
- To perform regex, we can use the 're' library in python.

## 6. Edit distance
Edit distance gives us a way to quantify both of these intuitions about string similarity. More formally, the minimum edit distance between two strings is defined as the minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another.

## 7. Encoding
Encoding refers to the process of converting text data, typically in the form of tokens, into numerical representations that can be used as input for machine learning models. Tokens are usually words, subwords, or characters, depending on the level of granularity chosen for tokenization. Tokenization is a crucial preprocessing step in NLP that helps convert raw text into a format suitable for analysis and further processing. 

- **Encoding:** This is the process of converting categorical variables into numerical values. The goal is to represent each category with a unique number so that algorithms can understand them. 

- **Vectorization:** This is the process of transforming textual data into numerical vectors. Text data needs to be converted into numerical form before it can be fed into machine learning models.


### Encoder
`The encoder component converts the preprocessed text into embeddings.`It processes the input sequence element by element, utilizing either recurrent layers or self-attention mechanisms to create this representation. The encoder's output serves as a rich contextual representation that can be harnessed for numerous downstream tasks.

### Decoder
The decoder takes this fixed-size representation(generally numerical) generated by the encoder and employs it to produce an output sequence, such as a translation in a target language. Like the encoder, it operates step by step and generates one token at a time, with each generation dependent on the prior tokens. It sequences autoregressively - using preceding outputs as context for subsequent token generation. 

## 8. Vocabulory
Vocabulary refers to the set of unique words, tokens, or subword units that a language model is designed to understand and work with. Vocabulory can be character level or word level.

## 9. TF-IDF Score
TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical `statistic used to reflect how important a word is to a document in a collection fo documents.` It is often used in information retrieval and text mining to estimate the relevance of a document to a given search query.

`TF(for a word in a document d1) = COUNT(word) in d1 / COUNT(d)`

`IDF(for a word) = loge(COUNT(d) / COUNT(d) with 'word' in it)`

`TF-IDF Score(for a word in document d1) = TF(word, d1) * IDF(word)`

## 10. PMI
An `alternative weighting function to tf-idf`, PPMI (positive pointwise mutual information), is used for term-term-matrices, when the vector dimensions correspond to words rather than documents. PPMI draws on the intuition that the best way to weigh the association between two words is to ask how much more the two words co-occur in our corpus than we would have a priori expected them to appear by chance.

If x is a word and y be a context word, then 

    PMI(x, y) = log2[p(x, y)/(p(x)*p(y))]
The numerator tells us how often we observed the two words together and the denominator tells us how often we would expect the two words to co-occur assuming they each occurred independently.

### POS tagging
Parts of speech tagging is done to tag the parts of speech in a sentence.

## NER
Named entity recognition is done to find out proper nouns, etc which are usually not found in the vocabulory. Eg: Tony, Mumbai, 

