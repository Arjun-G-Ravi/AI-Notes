## Sequence Models
Sequence models are a type of machine learning model used for tasks involving sequences of data, where the order and dependencies between elements in the sequence matter. They are used a lot for natural language processing.

## Language Model
A language model is a type of artificial intelligence model that is designed to understand and generate human language. Language models are a fundamental part of natural language processing (NLP) and have a wide range of applications in tasks such as text generation, machine translation, sentiment analysis, and more. The primary goal of a language model is to capture the statistical and contextual properties of language, allowing it to make predictions about what words or phrases are likely to come next in a given text.

## Tokenization 
Tokenization is the process of breaking down a sequence of text into smaller units, such as words or subword units, known as tokens.

## Vocabulory
In the context of language models and natural language processing (NLP), vocabulary, often referred to as the "vocabulary size" or "lexicon," refers to the set of unique words, tokens, or subword units that a language model is designed to understand and work with. This vocabulory can be character level or word level.

## Encoder
The encoder's role is to take an input sequence, such as a sentence in a source language, and transform it into a fixed-size representation, often referred to as a "context" or "hidden state." It processes the input sequence element by element, utilizing either recurrent layers or self-attention mechanisms to create this representation. The encoder's output serves as a rich contextual representation that can be harnessed for numerous downstream tasks.

## Decoder
The decoder takes this fixed-size representation generated by the encoder and employs it to produce an output sequence, such as a translation in a target language. Like the encoder, it operates step by step and generates one token at a time, with each generation dependent on the prior tokens. It sequences autoregressively - using preceding outputs as context for subsequent token generation. 
