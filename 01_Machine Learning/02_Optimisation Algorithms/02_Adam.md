# Adam (Adaptive Moment Estimation)
Adam, short for "Adaptive Moment Estimation," is an optimization algorithm commonly used to do backprop to train deep neural networks. The key features of Adam are adaptive learning rates and momentum-like behavior. It adapts the learning rates for each parameter based on the magnitude of the historical gradients (captured by the second moment) and incorporates momentum by using the first moment. Adam has gained popularity because it often converges faster and performs well in practice for a wide range of deep learning tasks. 
