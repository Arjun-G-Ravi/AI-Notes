{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = '/home/arjun/Documents/ModelSaves/GPT2Alpaca'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "ERROR: /home/arjun/NewPytorchEnv/bin/python3.10: undefined symbol: cudaRuntimeGetVersion\n",
      "CUDA SETUP: libcudart.so path is None\n",
      "CUDA SETUP: Is seems that your cuda installation is not in your path. See https://github.com/TimDettmers/bitsandbytes/issues/85 for more information.\n",
      "CUDA SETUP: CUDA version lower than 11 are currently not supported for LLM.int8(). You will be only to use 8-bit optimizers and quantization routines!!\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 00\n",
      "CUDA SETUP: Loading binary /home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('1'), PosixPath('0')}\n",
      "  warn(msg)\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('@/tmp/.ICE-unix/1600,unix/PC'), PosixPath('local/PC')}\n",
      "  warn(msg)\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/etc/xdg/xdg-ubuntu')}\n",
      "  warn(msg)\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/var/lib/flatpak/exports/share')}\n",
      "  warn(msg)\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer,GPT2LMHeadModel, get_scheduler \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset preparation and tokenising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tatsu-lab--alpaca-2b32f0433506ef5f\n",
      "Found cached dataset parquet (/home/arjun/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5cfc6d75d6e4768b821e97a586b5479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"tatsu-lab/alpaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': Value(dtype='string', id=None),\n",
       " 'input': Value(dtype='string', id=None),\n",
       " 'output': Value(dtype='string', id=None),\n",
       " 'text': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text'],\n",
       "    num_rows: 52000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset['train']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Give three tips for staying healthy.',\n",
       " 'input': '',\n",
       " 'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.',\n",
       " 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dataset smaller for fast training\n",
    "dataset = dataset.select(range(52000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For loading model and tokeniser from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For loading tokeniser from save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(save_loc)\n",
    "model =   GPT2LMHeadModel.from_pretrained(save_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = {'input_ids':[], 'attention_mask':[]}\n",
    "\n",
    "for example in dataset:\n",
    "    input_text = example['text'].replace('###','')   \n",
    "    encoded_data = tokenizer('<|startoftext|>' + input_text + '<|endoftext|>',truncation=True, max_length=768, padding=\"max_length\")\n",
    "    new_dataset['input_ids'].append(encoded_data['input_ids'])\n",
    "    new_dataset['attention_mask'].append(encoded_data['attention_mask'])\n",
    "\n",
    "new_dataset = Dataset.from_dict(new_dataset)\n",
    "new_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(new_dataset,shuffle=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimiser and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 14\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_training_steps = num_epochs * len(dataloader)\n",
    "lr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266fd161ed804ac19ef8c4dbb0f86d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1749 [00:00<?, ?steps/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Average training loss =  0.27909325844049454\n",
      "Epoch: 2 Average training loss =  0.23163198763132095\n",
      "Epoch: 3 Average training loss =  0.21218926632404328\n",
      "Epoch: 4 Average training loss =  0.1976565789580345\n",
      "Epoch: 5 Average training loss =  0.18594333910942076\n",
      "Epoch: 6 Average training loss =  0.17401826828718187\n",
      "Epoch: 7 Average training loss =  0.16392649263143538\n",
      "Epoch: 8 Average training loss =  0.15621806234121322\n",
      "Epoch: 9 Average training loss =  0.14885319823026658\n",
      "Epoch: 10 Average training loss =  0.14225157314538955\n",
      "Epoch: 11 Average training loss =  0.13674755388498305\n",
      "Epoch: 12 Average training loss =  0.13316831189393996\n",
      "Epoch: 13 Average training loss =  0.1299300209879875\n",
      "Epoch: 14 Average training loss =  0.1280406168103218\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps-1),desc='Training', unit='steps')\n",
    "model.train()   # Some layers behave differently to training and inference. This sets all those \n",
    "                 # layers into training mode\n",
    "ep = 0\n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss=0\n",
    "    for batch in dataloader:\n",
    "        # print(batch)\n",
    "        batch_data = batch['input_ids'].to(device)\n",
    "        attention = batch['attention_mask'].to(device)\n",
    "        # batch_data = batch_data.flatten()  # Flatten the tensor to 1-dimensional\n",
    "        # outputs = model.generate(batch_data.unsqueeze(0))\n",
    "        model.zero_grad()  \n",
    "        outputs = model(  batch_data,\n",
    "                          labels=batch_data, \n",
    "                          attention_mask = attention,\n",
    "                          token_type_ids=None\n",
    "                        )\n",
    "\n",
    "        # metric.add_batch(predictions=predictions, references=batch[\"input_ids\"])\n",
    "        loss = outputs[0]     # compute loss \n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "        loss.backward()          # computes gradients\n",
    "        optimizer.step()         # optimises\n",
    "        lr_scheduler.step()      # updates lr according to schedule. Improves performance\n",
    "        optimizer.zero_grad()    # resets the gradients\n",
    "        progress_bar.update(1)   # updates progress bar by 1\n",
    "    avg_train_loss = total_train_loss / len(dataloader)\n",
    "    ep += 1\n",
    "    print('Epoch:',ep,'Average training loss = ',avg_train_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving model and tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/arjun/Documents/ModelSaves/GPT2Alpaca.pt/tokenizer_config.json',\n",
       " '/home/arjun/Documents/ModelSaves/GPT2Alpaca.pt/special_tokens_map.json',\n",
       " '/home/arjun/Documents/ModelSaves/GPT2Alpaca.pt/vocab.json',\n",
       " '/home/arjun/Documents/ModelSaves/GPT2Alpaca.pt/merges.txt',\n",
       " '/home/arjun/Documents/ModelSaves/GPT2Alpaca.pt/added_tokens.json',\n",
       " '/home/arjun/Documents/ModelSaves/GPT2Alpaca.pt/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(save_loc)\n",
    "tokenizer.save_pretrained(save_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=200) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------\n",
      "<-1->            ‘Duck is king                                                                                                                                                                                        \n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------\n",
      "<-2-> _______Do you remember when it was all sweet and sweet? When the cows came home, they were all so happy. _______But when they came home, they were all so sad. So why did they come back? They were not giving back what they had given them. And that's when they began to cry. They began to speak, telling stories, sharing food and companionship. Then they were like this: they were going to go back to their old ways and do what they loved. And that's when they were free to do whatever they wanted. So how did they survive that sad fate?\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------\n",
      "<-3-> ________ _______                                                                                                                                                                                                     \n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------\n",
      "<-4->  Little milk sweet to sweet to sweet.Tears are shed from my sweetheart's breasts.My heart is full of joy.\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------\n",
      "<-5-> ’Dear John, I’re writing a poem on your behalf of your beloved. I hope that it can help you understand my love for you and understand my resistance to rejection. It's not only because you're so kind and generous, but also because you're a part of my life and I'm willing to share this journey with you. Your generosity and understanding will enable me to explore the possibilities of your life and embrace the beauty of your innocence. I hope that you will find what I want and that you'll forgive me for being late for something I didn't deserve.\n"
     ]
    }
   ],
   "source": [
    "question = 'Write a poem on cow'\n",
    "\n",
    "prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    " Instruction:{question}\n",
    " Response: \"\"\"\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "                                generated, \n",
    "                                do_sample=True,   \n",
    "                                top_k=100,\n",
    "                                max_length=100,\n",
    "                                max_new_tokens=200,\n",
    "                                top_p=.95, \n",
    "                                num_return_sequences= 5,\n",
    "                                temperature = .9,\n",
    "                                )\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    ans = tokenizer.decode(sample_output, skip_special_tokens=True).split('Response: ')\n",
    "    print(\"\\n\\n-------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "    try:        print(f'<-{i+1}-> {ans[1]}')\n",
    "    except:\n",
    "        print(f'<-{i+1}-> ___No response___')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewPytorchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
