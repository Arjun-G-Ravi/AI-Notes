# NLP
Natural Language Processing is a field of study focused on the interaction between computers and human (natural) languages. The ultimate objective of NLP is to read, decipher, understand, and make sense of the human language in a valuable way.

## Sequence Models
Sequence models are a type of machine learning model used for tasks involving sequences of data, where the order and dependencies between elements in the sequence matter. They are used a lot for natural language processing.

## Language Model
A language model is a type of artificial intelligence model that is designed to understand and generate human language. Language models are a fundamental part of natural language processing (NLP) and have a wide range of applications in tasks such as text generation, machine translation, sentiment analysis, and more. 

`Some important NLP related concepts are:`
## Text normalization
Text normalization in Natural Language Processing (NLP) refers to the process of converting a piece of text into a standardized form. This involves correcting spelling errors, expanding abbreviations and contractions, removing punctuation marks, converting all characters to lowercase or uppercase, and applying other transformations that make the text easier to analyze and compare.

## Tokenisation
Converting text data into tokens.
- Should "New York" be considered as one token or two? Should punctuations be considered to be separate tokens? etc. all depends on the application. 

## Stemming
Reducing words to their base or root form, by chopping off parts of words.
Eg: SnowBallStemmer(violent stemmer), PorterStemmer(simple stemmer)

## Lemmatization
It is stemming performed by consulting the context of the text

## Regular Expression
Formally, a regular expression is an algebraic notation for characterizing a set of strings. Regular expressions are particularly useful for searching in texts, when we have a pattern to search for and a corpus of texts to search through.
- To perform regex, we can use the 're' library in python.

# Edit distance
Edit distance gives us a way to quantify both of these intuitions about string similarity. More formally, the minimum edit distance between two strings is defined as the minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another.

# Encoding
Encoding refers to the process of converting text data, typically in the form of tokens, into numerical representations that can be used as input for machine learning models. Tokens are usually words, subwords, or characters, depending on the level of granularity chosen for tokenization. Tokenization is a crucial preprocessing step in NLP that helps convert raw text into a format suitable for analysis and further processing. 

- **Encoding:** This is the process of converting categorical variables into numerical values. The goal is to represent each category with a unique number so that algorithms can understand them. 

- **Vectorization:** This is the process of transforming textual data into numerical vectors. Text data needs to be converted into numerical form before it can be fed into machine learning models.

## Vocabulory
Vocabulary refers to the set of unique words, tokens, or subword units that a language model is designed to understand and work with. Vocabulory can be character level or word level.

## Encoder
The primary function of an encoder is to transform input data, often in the form of text or sequences, into a numerical representation that can be processed by the model. It processes the input sequence element by element, utilizing either recurrent layers or self-attention mechanisms to create this representation. The encoder's output serves as a rich contextual representation that can be harnessed for numerous downstream tasks.

## Decoder
The decoder takes this fixed-size representation(generally numerical) generated by the encoder and employs it to produce an output sequence, such as a translation in a target language. Like the encoder, it operates step by step and generates one token at a time, with each generation dependent on the prior tokens. It sequences autoregressively - using preceding outputs as context for subsequent token generation. 
