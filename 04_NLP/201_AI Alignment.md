# AI ALIGNMENT
AI alignment, also known as value alignment or AI safety, is a subfield of artificial intelligence research that focuses on ensuring that advanced artificial intelligence (AI) systems are aligned with human values and goals.

LLMs are big black boxes. It is very hard to understand why it said the thing that it did. As AI is getting more and more intelligent, it is essential that it shares some basic ideologies(basic human values, etc.) with us human beings. The process of aligning the AI towards human goals is called AI alignment.

## LLM Interpretability
AI interpretability(AI explainability) is the ability of an artificial intelligence to provide clear and understandable explanations for its decisions, predictions, or recommendations.

If it was possible to reverse engineer the parameters of trained neural networks into human-understandable algorithms, it may enable us to catch safety problems the same way we are able to in code. Thus AI interpretability is the first step towards AI alignment.

---