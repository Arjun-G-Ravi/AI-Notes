# Improving Language Understanding by Generative Pre-Training
###### by Alec Radford, Karthik Narasimhan, Tim Salimans,  Ilya Sutskever

Paper available at: https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf

This paper introduces the common approach of two step LLM training - pretraining(on large unlabelled text via self-supervised learning) and fine tuning(on a smaller, labelled and cleaner dataset, often on a particular domain via supervised learning). Nowadays, this approach has become the golden standard for LLM training.