{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiSJY0k-X-6R"
      },
      "source": [
        "Colab Upgrades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGuVRXP1X-6V",
        "outputId": "067a5dca-2534-488f-9fa0-19f5b3045e43"
      },
      "outputs": [],
      "source": [
        "# !pip install PyPDF2 --q\n",
        "# !pip install transformers --q\n",
        "# !apt-get install -y poppler-utils --q\n",
        "# !pip install transformers networkx --q\n",
        "# !transformers-cli cache clear --q\n",
        "# !pip install newspaper3k\n",
        "# "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmIA-CrxX-6X"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpBLBL_lX-6X",
        "outputId": "4ed39623-f701-4b0c-f849-0241c7699420"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/arjun/NewPytorchEnv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[nltk_data] Downloading package punkt to /home/arjun/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import PyPDF2\n",
        "from io import StringIO\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import torch\n",
        "from transformers import LongformerTokenizer, LongformerModel, pipeline\n",
        "import nltk\n",
        "import re\n",
        "import newspaper\n",
        "import warnings\n",
        "nltk.download('punkt')\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# import spacy\n",
        "# import networkx as nx\n",
        "# import textract\n",
        "# from spacy import displacy\n",
        "# import textacy\n",
        "# import sacremoses as sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv7CLufDYM4I"
      },
      "source": [
        "Check for CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezpKPwVoYKus",
        "outputId": "fd9af9b5-c137-4086-be96-7c673675da5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "# The following output must be cuda to ensure that the GPU is used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFVR79obX-6Y"
      },
      "source": [
        "Sentence Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cFc5cornX-6Z"
      },
      "outputs": [],
      "source": [
        "def split_into_sentences(paragraph):\n",
        "    sentences = nltk.sent_tokenize(paragraph)\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In_UzjAMX-6Z"
      },
      "source": [
        "Bart-large-cnn-samsum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wbO5lfNfX-6Z"
      },
      "outputs": [],
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"philschmid/bart-large-cnn-samsum\", device='cuda:0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "429IWw0gX-6a"
      },
      "source": [
        "Longformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjT_mnWAX-6a",
        "outputId": "6ca7c276-96d8-46ac-8b96-8c9ead8276c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded longformer to device\n"
          ]
        }
      ],
      "source": [
        "# Initialize Longformer tokenizer and model\n",
        "longformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "longformer_model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
        "\n",
        "# Move the model to the GPU\n",
        "longformer_model.to(device)\n",
        "print(\"Loaded longformer to device\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_8YHq-GX-6b"
      },
      "source": [
        "For counting words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_SxT70AEX-6b"
      },
      "outputs": [],
      "source": [
        "def count_words(sentence):\n",
        "    return len(re.findall(r\"\\w+\", sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpVGdFWNX-6b"
      },
      "source": [
        "Spacy Entity recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LHcFk5K6X-6c"
      },
      "outputs": [],
      "source": [
        "# Load the entity recognition pipeline\n",
        "# nlp = spacy.load(\"en_core_web_lg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEIJ-6DaX-6c"
      },
      "source": [
        "Read funtions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Sl9Hf4gCX-6c"
      },
      "outputs": [],
      "source": [
        "# Function to read and extract text from a PDF document\n",
        "def read_pdf(file_path):\n",
        "    if not os.path.isfile(file_path) or not file_path.endswith('.pdf'):\n",
        "        raise ValueError('Invalid file path or file format. Please provide a valid PDF file.')\n",
        "\n",
        "    from pdfminer.high_level import extract_text\n",
        "    text = extract_text(file_path)\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "# Function to read and extract text from a text document\n",
        "def read_text(file_path):\n",
        "    if not os.path.isfile(file_path) or not file_path.endswith('.txt'):\n",
        "        raise ValueError('Invalid file path or file format. Please provide a valid text file.')\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "        return text\n",
        "\n",
        "# Function to read and extract text from a website URL\n",
        "def read_url(url):\n",
        "    try:\n",
        "        def get_main_text_from_website(url):\n",
        "            article = newspaper.Article(url)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "            return article.text\n",
        "\n",
        "        # Example usage\n",
        "        # url = 'https://www.example.com'\n",
        "        main_text = get_main_text_from_website(url)\n",
        "        return main_text\n",
        "    except:\n",
        "        raise ValueError('Invalid URL or unable to extract text from URL. Please provide a valid website URL.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fA2H7WGX-6d"
      },
      "source": [
        "Extractive summarisation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Bk1bYYzsX-6d"
      },
      "outputs": [],
      "source": [
        "def extractive_summarization(input_text):\n",
        "    in_words = count_words(input_text)\n",
        "    if in_words<500:\n",
        "        out_sentence = int(in_words/50)\n",
        "    else:\n",
        "        out_sentence = int(in_words/30)\n",
        "\n",
        "\n",
        "    sentences = split_into_sentences(input_text)\n",
        "    hierarchical_summary = []\n",
        "    count = 0\n",
        "    for chunk in sentences:\n",
        "        input_ids = longformer_tokenizer.encode(chunk, return_tensors='pt').to(device)\n",
        "        outputs = longformer_model(input_ids=input_ids)\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        sentence_embeddings = hidden_states[0]\n",
        "        sentence_scores = torch.matmul(sentence_embeddings, sentence_embeddings.T)\n",
        "        sentence_scores = sentence_scores.squeeze(0).tolist()\n",
        "        if not chunk:\n",
        "            continue  # Skip empty chunks\n",
        "\n",
        "        hierarchical_summary.append((count,chunk, sentence_scores))\n",
        "        count +=1\n",
        "\n",
        "    hierarchical_summary.sort(key=lambda x: x[1], reverse=True)  # Sort sentences by score\n",
        "    num_summary_tokens = 0\n",
        "    summary = []\n",
        "    for ct,sentence, score in hierarchical_summary:\n",
        "        if num_summary_tokens + len(longformer_tokenizer.tokenize(sentence)) <= 50000:\n",
        "            summary.append([ct,sentence])\n",
        "            num_summary_tokens += len(longformer_tokenizer.tokenize(sentence))\n",
        "        else:\n",
        "            break\n",
        "    summary = summary[:out_sentence]\n",
        "    sorted_data = sorted(summary, key=lambda x: x[0])\n",
        "    extractive_result = []\n",
        "    for i in sorted_data:\n",
        "        extractive_result.append(i[1])\n",
        "    out = ' '.join(extractive_result)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuVcD95WX-6d"
      },
      "source": [
        "Abstractive summarisation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_XT8BUFxX-6d"
      },
      "outputs": [],
      "source": [
        "def abstractive_summarization(input_text):\n",
        "    num_of_sentence_per_chunk = 10\n",
        "    sentences = split_into_sentences(input_text)\n",
        "    sentence_chunks = []\n",
        "    summarised_output = ''\n",
        "    ct = 0\n",
        "    while ct<=len(sentences):\n",
        "        chunk = sentences[ct: ct+num_of_sentence_per_chunk]\n",
        "        chunk = ' '.join(chunk)\n",
        "        if len(chunk) > 700:    # Little data in the tail-end will lead to super-bad summarisation\n",
        "            sentence_chunks.append(chunk)\n",
        "        ct += num_of_sentence_per_chunk\n",
        "    # length = len(sentence_chunks)\n",
        "    progress = 0\n",
        "    for chunk in sentence_chunks:\n",
        "        if len(chunk) < 10:\n",
        "            continue\n",
        "        data = summarizer(chunk,min_length = 150, max_length=200)[0]['summary_text']\n",
        "        summarised_output += data\n",
        "        # print(f\"Progress = {progress+1}/{length}\")\n",
        "        progress+=1\n",
        "    return summarised_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ridGY_eX-6e"
      },
      "source": [
        "knowledge graph-guided summarization funtions -- In progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JR53wBihqrfy"
      },
      "outputs": [],
      "source": [
        "def extract_relations(text):\n",
        "    doc = nlp(text)\n",
        "    relations = []\n",
        "    \n",
        "    for sent in doc.sents:\n",
        "        for token in sent:\n",
        "            if token.dep_ == \"ROOT\" and token.pos_ == \"VERB\":\n",
        "                subject = [t for t in token.children if t.dep_ in [\"nsubj\", \"nsubjpass\"]]\n",
        "                object_ = [t for t in token.children if t.dep_ in [\"dobj\", \"pobj\"]]\n",
        "                if subject and object_:\n",
        "                    relations.append((subject[0].text, token.text, object_[0].text))\n",
        "    return relations\n",
        "\n",
        "def create_knowledge_graph(input_text):\n",
        "    relations = extract_relations(input_text)\n",
        "    \n",
        "    graph = nx.Graph()\n",
        "    for subject, verb, obj in relations:\n",
        "        graph.add_node(subject, label=\"ENTITY\")\n",
        "        graph.add_node(obj, label=\"ENTITY\")\n",
        "        graph.add_edge(subject, obj, relation=verb)\n",
        "    return graph\n",
        "\n",
        "# Function to perform knowledge graph-guided summarization\n",
        "def knowledge_graph_guided_summarization(input_text):\n",
        "    # Step 1: Create knowledge graph\n",
        "    graph = create_knowledge_graph(input_text)\n",
        "\n",
        "    # Step 2: Perform extractive summarization using knowledge graph\n",
        "    summary_sentences = []\n",
        "    for node in graph.nodes(data=True):\n",
        "        if node[1]['label'].startswith('Ġ'):\n",
        "            for edge in graph.edges(nbunch=node[0], data=True):\n",
        "                if edge[2]['relation'] in ['attr', 'dobj', 'prep']:\n",
        "                    if graph.nodes[edge[1]]['label'].startswith('Ġ'):\n",
        "                        summary_sentences.append(graph.nodes[edge[1]]['label'])\n",
        "    summary_sentences = list(set(summary_sentences))\n",
        "    summary_sentences.sort(key=lambda s: input_text.find(s))\n",
        "    summary_text = '. '.join(summary_sentences)\n",
        "\n",
        "    # Step 3: Perform abstractive summarization on the extracted summary\n",
        "    summary = abstractive_summarization(summary_text)\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY_bxXN8X-6e"
      },
      "source": [
        "Summarisation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XwEESf4FX-6e"
      },
      "outputs": [],
      "source": [
        "def summarize_file(file_path_or_url):\n",
        "    # Check if the input is a file path or a website URL\n",
        "    if os.path.isfile(file_path_or_url):\n",
        "        file_path = file_path_or_url\n",
        "        # Read and extract text from the input file\n",
        "        if file_path.endswith('.pdf'):\n",
        "            text = read_pdf(file_path)\n",
        "        elif file_path.endswith('.txt'):\n",
        "            text = read_text(file_path)\n",
        "        else:\n",
        "            raise ValueError('Invalid file format. Please provide a valid PDF or text file.')\n",
        "    else:\n",
        "        # Read and extract text from the website URL\n",
        "        text = read_url(file_path_or_url)\n",
        "\n",
        "    print('TEXT: -------------------\\n',text,'\\n-----------------------------')\n",
        "\n",
        "    # Perform extractive summarization\n",
        "    extractive_summary = extractive_summarization(text)\n",
        "    \n",
        "    # Perform abstractive summarization\n",
        "    abstractive_summary = abstractive_summarization(text)\n",
        "\n",
        "    # graph = create_knowledge_graph(text)\n",
        "    # print(graph.nodes())\n",
        "    # print(graph.edges())\n",
        "    # Perform knowledge graph-guided summarization on the extracted text\n",
        "    # summary = knowledge_graph_guided_summarization(text)\n",
        "    return abstractive_summary, extractive_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVfzFEzyX-6f"
      },
      "source": [
        "Summary generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yG0uTEP4X-6f",
        "outputId": "e3e6b991-1c44-4114-f0d4-e825f064a832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEXT: -------------------\n",
            " EARTH EATS\n",
            "\n",
            "We try to bridge the fact that 40% of food produced is wasted while approximately 14.8% of the\n",
            "people remain undernourished. We will provide an interface via an app for people to buy and\n",
            "sell excess food from large functions or events thus reducing the practice of using food after\n",
            "long refrigeration and financial stress on food manufacturers by reducing storage cost and We\n",
            "hire employees on independent contracts whose wages are based on the number of deliveries\n",
            "and thus act as a part time job opportunity.\n",
            "\n",
            "Vendors can sell excess Fruits and vegetables at reduced cost before it spoils. This reduces\n",
            "stress on these vendors preventing excess use of pesticides and ensures minimum profit.\n",
            "\n",
            "This app helps producers trade surplus food at functions at 50-60% cost, preventing food\n",
            "wastage or free distribution giving enough returns. A worker depending on daily wage to win\n",
            "bread when falls sick and cannot go to work can avail food from a base price of 40% via this\n",
            "platform. Via this app we can reduce the expenditure of buying food from external sources;\n",
            "reduce the public hunger in some scenarios and reduce the use of refrigerators promoting\n",
            "greener earth.\n",
            "\n",
            "Even though we have competitors like ShareTheMeal, FlashFood, Zomato etc. We have an\n",
            "advantage because we address both food waste and malnourishment, have wider product\n",
            "coverage, handle cooked food, and provide information about local food resources, while others\n",
            "only address one or the other. We work based on the model that is integrated with the ideas of\n",
            "the above three apps. It integrates the employment scheme of Zomato and the operational\n",
            "scheme of two other apps. Its revenue scheme follows that of Amazon. With expansion in\n",
            "service and geographic coverage, there is potential for significant profit increase and entry into\n",
            "the share market.\n",
            "\n",
            "\f \n",
            "-----------------------------\n"
          ]
        }
      ],
      "source": [
        "file_path = 'EARTH EATS.pdf'\n",
        "abstractive_summary, extractive_summary = summarize_file(file_path)\n",
        "extractive_summary = '.'.join(extractive_summary.split('\\n'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extractive Summary:\n",
            "We will provide an interface via an app for people to buy and.sell excess food from large functions or events thus reducing the practice of using food after.long refrigeration and financial stress on food manufacturers by reducing storage cost and We.hire employees on independent contracts whose wages are based on the number of deliveries.and thus act as a part time job opportunity. Vendors can sell excess Fruits and vegetables at reduced cost before it spoils. Via this app we can reduce the expenditure of buying food from external sources;.reduce the public hunger in some scenarios and reduce the use of refrigerators promoting.greener earth. We have an.advantage because we address both food waste and malnourishment, have wider product.coverage, handle cooked food, and provide information about local food resources, while others.only address one or the other. We work based on the model that is integrated with the ideas of.the above three apps. With expansion in.service and geographic coverage, there is potential for significant profit increase and entry into.the share market.\n",
            "\n",
            "-------------------------------------------------------------------------------------\n",
            "Abstractive Summary:\n",
            " EARTH EATS will provide an interface via an app for people to buy and sell excess food from large functions or events. 40% of food produced is wasted and 14.8% of people remain undernourished. This app helps producers trade surplus food at functions at 50-60% cost, preventing food waste or free distribution. A worker depending on daily wage to winbread when he falls sick and cannot go to work can avail food from a base price of 40% via this platform. This reduces the expenditure of buying food from external sources, reduces the public hunger and reduces the use of refrigerators promoting greener earth. There are competitors like ShareTheMeal, FlashFood, Zomato etc. but Earth Eat has an advantage because it addresses both food waste and malnourishment.\n"
          ]
        }
      ],
      "source": [
        "print(\"Extractive Summary:\")\n",
        "print(extractive_summary)\n",
        "print(\"\\n-------------------------------------------------------------------------------------\\nAbstractive Summary:\")\n",
        "print(abstractive_summary)\n",
        "# print(\"\\nThe hybrid method Summary is:\")\n",
        "# print(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBlo75hicaQC"
      },
      "source": [
        "# Word Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ci5AtUteX-6f",
        "outputId": "91f97f7f-6333-4da1-b8a4-daff7f7dbce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words in Input text is: 302\n",
            "Words in Extractive summary is: 180\n",
            "Words in Abstractive summary is: 128\n"
          ]
        }
      ],
      "source": [
        "if os.path.isfile(file_path):\n",
        "    file_path = file_path\n",
        "    # Read and extract text from the input file\n",
        "    if file_path.endswith('.pdf'):\n",
        "        in_text = read_pdf(file_path)\n",
        "    elif file_path.endswith('.txt'):\n",
        "        in_text = read_text(file_path)\n",
        "    else:\n",
        "        raise ValueError('Invalid file format. Please provide a valid PDF or text file.')\n",
        "else:\n",
        "    # Read and extract text from the website URL\n",
        "        \n",
        "        in_text = read_url(file_path)\n",
        "print(f\"Words in Input text is: {count_words(in_text)}\")\n",
        "print(f\"Words in Extractive summary is: {count_words(extractive_summary)}\")\n",
        "print(f\"Words in Abstractive summary is: {count_words(abstractive_summary)}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
