## Sequence Models
Sequence models are a type of machine learning model used for tasks involving sequences of data, where the order and dependencies between elements in the sequence matter. They are used a lot for natural language processing.

## Language Model
A language model is a type of artificial intelligence model that is designed to understand and generate human language. Language models are a fundamental part of natural language processing (NLP) and have a wide range of applications in tasks such as text generation, machine translation, sentiment analysis, and more. The primary goal of a language model is to capture the statistical and contextual properties of language, allowing it to make predictions about what words or phrases are likely to come next in a given text.

All language models have a vocabulary, which is a predefined set of words, subwords, or tokens that the model has learned during training. A language model looks at the input sequence, and predicts the word with the highest probability from the vocabulory.


## Tokenization 
Tokenization is the process of breaking down a sequence of text into smaller units, such as words or subword units, known as tokens.

## Encoding
Encoding refers to the process of converting text data, typically in the form of tokens, into numerical representations that can be used as input for machine learning models. Tokens are usually words, subwords, or characters, depending on the level of granularity chosen for tokenization. Tokenization is a crucial preprocessing step in NLP that helps convert raw text into a format suitable for analysis and further processing. 

## Vocabulory
Vocabulary refers to the set of unique words, tokens, or subword units that a language model is designed to understand and work with. Vocabulory can be character level or word level.

## Encoder
The primary function of an encoder is to transform input data, often in the form of text or sequences, into a numerical representation that can be processed by the model. It processes the input sequence element by element, utilizing either recurrent layers or self-attention mechanisms to create this representation. The encoder's output serves as a rich contextual representation that can be harnessed for numerous downstream tasks.

## Decoder
The decoder takes this fixed-size representation(generally numerical) generated by the encoder and employs it to produce an output sequence, such as a translation in a target language. Like the encoder, it operates step by step and generates one token at a time, with each generation dependent on the prior tokens. It sequences autoregressively - using preceding outputs as context for subsequent token generation. 
