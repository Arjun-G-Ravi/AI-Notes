# NLP CONCEPTS
Some random conepts!

## Edit distance
Edit distance gives us a way to quantify both of these intuitions about string similarity. More formally, the minimum edit distance between two strings is defined as the minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another.

## Encoding
Encoding refers to the process of converting text data, typically in the form of tokens, into numerical representations that can be used as input for machine learning models. Tokens are usually words, subwords, or characters, depending on the level of granularity chosen for tokenization. Tokenization is a crucial preprocessing step in NLP that helps convert raw text into a format suitable for analysis and further processing. 

- **Encoding:** This is the process of converting categorical variables into numerical values. The goal is to represent each category with a unique number so that algorithms can understand them. 

- **Vectorization:** This is the process of transforming textual data into numerical vectors. Text data needs to be converted into numerical form before it can be fed into machine learning models.

### Encoder
`The encoder component converts the preprocessed text into embeddings(a fixed size representation of the input data).`It processes the input sequence element by element, and convert them into numerical representations. The encoder's output serves as a rich contextual representation that can be harnessed for furthur text processing.

```
- The encoder layer helps to create contextualized representations of the input sequence. It processes the input sequence in a self-attention mechanism, which allows the model to capture long-range dependencies and relationships between different parts of the input. In a decoder-only model, the input sequence is not processed in the same way, and the model may not be able to capture these contextual relationships as effectively.

- The encoder layer is responsible for processing the input sequence and generating a representation that is independent of the specific task or output. This separation of concerns allows the model to be more modular and flexible, making it easier to adapt to different tasks or output formats.
In a decoder-only model, the model is tightly coupled to the specific task or output, which can make it more difficult to adapt to new tasks or output formats.

- The encoder layer can be parallelized more easily than the decoder layer, since the self-attention mechanism can be computed in parallel across different parts of the input sequence. 
```

### Decoder
The decoder takes this fixed-size representation(generally numerical) generated by the encoder and employs it to produce an output sequence, such as a translation in a target language. Like the encoder, it operates step by step and generates one token at a time, with each generation dependent on the prior tokens. It sequences autoregressively - using preceding outputs as context for subsequent token generation. 

## Vocabulory
Vocabulary refers to the set of unique words, tokens, or subword units that a language model is designed to understand and work with. Generally, a token < UNK > is added to the vocabulory to handle unknown words.

## TF-IDF Score
TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical `statistic used to reflect how important a word is to a document in a collection fo documents.` It is often used in information retrieval and text mining to estimate the relevance of a document to a given search query.

`TF(for a word in a document d1) = COUNT(word) in d1 / COUNT(d)`

`IDF(for a word) = loge(COUNT(d) / COUNT(d) with 'word' in it)`

`TF-IDF Score(for a word in document d1) = TF(word, d1) * IDF(word)`

## PPMI
An `alternative weighting function to tf-idf`, PPMI (positive pointwise mutual information), is used for term-term-matrices, when the vector dimensions correspond to words rather than documents. PPMI draws on the intuition that the best way to weigh the association between two words is to ask how much more the two words co-occur in our corpus than we would have a priori expected them to appear by chance.

If x is a word and y be a context word, then 

    PMI(x, y) = log2[p(x, y)/(p(x)*p(y))]
The numerator tells us how often we observed the two words together and the denominator tells us how often we would expect the two words to co-occur assuming they each occurred independently.