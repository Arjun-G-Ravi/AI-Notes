
## 3. Gated Recurrent Unit
![Alt text](<Screenshot from 2023-10-19 20-51-19.png>)

GRU (Gated Recurrent Unit) is a type of RNN architecture used in the field of NLP, designed to address some of the limitations of traditional RNNs, such as the vanishing gradient problem and the difficulty of capturing long-range dependencies in sequences.
GRUs help RNNs capture and remember important information, by having a memory of the past while also allowing them to adapt to new data as it comes in. `GRUs are actually a simplified LSTM model.`


## 5. Deep RNNs
Instead of recurring over a cell, we can recurr over a small NN. This creates deep RNNs.
![Alt text](<Screenshot from 2023-10-20 20-00-00.png>)
